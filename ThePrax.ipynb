{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umgebung vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/edadunashvili/ThePrax.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ThePrax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Korpus von Trainingsdaten erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " !!! Ein Typ bestimmen !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typNum='300'\n",
    "episode_string_train = \"a300_string_train.csv\"\n",
    "episode_roh_train = \"a300_roh_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorhandener gleichnamiger Korpus wird gelöscht "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(episode_string_train):\n",
    "    os.remove(episode_string_train)\n",
    "else:\n",
    "    print(\"Diese Datei existiert nicht\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Ordner \"Trainingsdaten\" nach den entsprechenden Textdateien suchen und in einer rohe Datei zusammentragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def word_to_lex(word):\n",
    "    ret=(word) \n",
    "    return ret\n",
    "\n",
    "def write_back(words):\n",
    "    with open(episode_roh_train,\"a\", encoding='utf-8') as output:\n",
    "        for word in words:\n",
    "            #print(word)\n",
    "            as_lex = word_to_lex(word[0])\n",
    "            full_word = '\"' + as_lex + '\"'\n",
    "            for sub_word in word[1:]:\n",
    "                full_word += \" , \"  '\"' + sub_word + '\"'\n",
    "            full_word +=\"\\n\"\n",
    "            output.write(full_word)\n",
    "\n",
    "def clean(line):\n",
    "    line = line.replace(\"\\n\",\" \").strip().lower()\n",
    "    line = line.replace(\"ä\",\"ae\").replace(\"ü\",\"ue\").replace(\"ö\",\"oe\").replace(\"ß\",\"ss\").replace(\",\",\"\").replace(\"«\",\"\")\n",
    "    line = line.replace(\"»\",\"\").replace(\".\",\"\").replace(\":\",\"\").replace(\";\",\"\").replace('\"',\"\")\n",
    "    line = line.replace(\"?\",\"\").replace(\"!\",\"\").replace(\"á\",\"a\").replace(\",\",\"\").replace(\"\\t\",\" \").replace(\"'\",\"\")\n",
    "    line = line.replace(\"‹\",\"\").replace(\"›\",\"\").replace(\"-\",\" \").replace(\"'('\",\"\").replace(\"')'\",\"\").replace('>','')\n",
    "    line = line.replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \").replace('–','').replace('—','').replace('<','')\n",
    "    line = line.replace(\"Â\", \"A\").replace(\"ø\", \"oe\").replace('“','').replace('„','').replace('(','').replace(')','')\n",
    "    line = line.replace(\"‚\", \"\").replace(']','').replace('[','')\n",
    "    if line == \"\": \n",
    "        return\n",
    "    \n",
    "    line=line.split(\"|\")\n",
    "    line[0]=line[0].split(\"|\")[0]\n",
    "    flex=[]\n",
    "    try:\n",
    "        flex=line[1].split(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    value=str(line)\n",
    "    line=str(line)   \n",
    "    flex.append(line)\n",
    "    ret=[]\n",
    "    for i in flex:\n",
    "        ret.append((i,value[0]))\n",
    "    return ret\n",
    "\n",
    "with open(episode_roh_train, \"w\", encoding='utf-8') as output:\n",
    "    output.write (\"quelle,episode,index_string,index_binar\\n\")\n",
    "pairs = []\n",
    "\n",
    "\n",
    "for file in glob.glob(\"Trainingsdaten/*.txt\"):\n",
    "    \n",
    "    if typNum in file:\n",
    "        with open(file, 'r', encoding='utf-8') as episode:\n",
    "            for line in episode.readlines():\n",
    "                clean_words = clean(line)\n",
    "                pairs = pairs + clean_words\n",
    "write_back(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rohdatei endgültig überarbeiten und in eine CSV Datei speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(episode_roh_train,'r', encoding ='utf-8')\n",
    "fout = open(episode_string_train, \"wt\", encoding ='utf-8')\n",
    "for kfz in fin:\n",
    "    fout.write(kfz.replace(', \"[\"',\"\").replace('\"[', \"\").replace(']\"',\"\").replace(\"', '\", \"','\").replace(\" '\", \"'\"))                      \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste von einmaligen Episoden aus der CSV Datei zeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(episode_string_train, encoding='utf-8')\n",
    "from collections import Counter\n",
    "indexliste=Counter(df.index_string)\n",
    "print(*indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell erstellen und trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import models\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Variable anpassen !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ziel_episode = 'e300_c_anfangssituation_ankunft_und_erkundigung_der_not'\n",
    "mini_frequenz = 2\n",
    "k_fach = 5\n",
    "layD = 96\n",
    "episode_string_train = 'a300_string_train.csv' \n",
    "episode_binar_train = 'a300_binar_train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufbereitung der Trainingsdaten\n",
    "Rohdaten laden und und den Episodenbestand betrachten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(episode_string_train,'r', encoding='utf-8') \n",
    "fout = open(episode_binar_train, \"wt\", encoding='utf-8')\n",
    "for efz in fin:\n",
    "    fout.write(efz.replace(\"'\",\"\"))  \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(episode_binar_train, encoding='utf-8')\n",
    "indexliste=Counter(df.index_string)\n",
    "print(*indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die gesuchte Episode etikettieren und den Rohdaten in Trainingsdaten umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(df.index_string):\n",
    "    if e == ziel_episode:\n",
    "        df.index_binar[i]='1'\n",
    "    else: \n",
    "        df.index_binar[i]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(episode_binar_train, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainngsdaten aufbereiten. Das Vorkommen jedes Wortes zählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "w = stopwords.words('german')\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['episode']),\n",
    "                       title='Vorkommen der Wörter zählen')\n",
    "for i, episode in enumerate(df['episode']): \n",
    "        pbar.update()\n",
    "        counts.update(episode.split())\n",
    "new_counts = {}\n",
    "for k, v in counts.items():\n",
    "    if v > mini_frequenz and k not in w:\n",
    "        new_counts[k] = v\n",
    "counts = Counter(new_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuordnung erzeugen und den verschiedenen Wörtern eindeutige Zahlen zuordnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts,0)}\n",
    "mapped_episoden = []\n",
    "pbar = pyprind.ProgBar(len(df['episode']),\n",
    "                       title='Episoden Zahlen zuordnen')\n",
    "for episode in df['episode']:\n",
    "    mapped_episoden.append([word_to_int.get(word) for word in episode.split()])\n",
    "    pbar.update()\n",
    "mapped_episoden = [list(filter(None, el)) for el in mapped_episoden]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traiingsndaten und Trainingslabels bestimmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mapped_episoden[0:] \n",
    "train_labels = df.loc[0:, 'index_binar'].values\n",
    "print(\"sequences =\",max([max(sequences) for sequences in mapped_episoden if len(sequences)>0]),\" \",\n",
    "      \"train_data =\", len (train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainingsdaten und -Labels vektorisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_laenge=max([max(sequences) for sequences in mapped_episoden if len(sequences)>0])\n",
    "def vectorize_sequences(sequences, dimension=sequences_laenge+1): \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "x_train = vectorize_sequences(train_data) \n",
    "y_train = np.asarray(train_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, element in enumerate(x_train[63]):\n",
    "    #print(i,'-', element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuronales Netz erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(layD, activation='tanh',\n",
    "                           input_shape=(sequences_laenge+1,)))\n",
    "    #model.add(layers.Dense(layD, activation='relu'))\n",
    "    model.add(layers.Dense(layD, activation='tanh')) \n",
    "    model.add(layers.Dense((1), activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuronales Netz trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = k_fach\n",
    "num_val_samples = len(x_train) // k \n",
    "num_epochs = 4\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_x_train = np.concatenate(\n",
    "        [x_train[:i * num_val_samples],\n",
    "         x_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_y_train = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    model.fit(partial_x_train, partial_y_train,\n",
    "              epochs=num_epochs, batch_size = 8, verbose=0)\n",
    "    # Evaluate the model on the validation data\n",
    "    val_mse, val_binary_accuracy = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_binary_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 8\n",
    "all_binary_accuracy_histories = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_x_train = np.concatenate(\n",
    "        [x_train[:i * num_val_samples],\n",
    "         x_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_y_train = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    history = model.fit(partial_x_train, partial_y_train,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size = 8, verbose=0)\n",
    "    binary_accuracy_history = history.history['val_binary_accuracy']\n",
    "    all_binary_accuracy_histories.append(binary_accuracy_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_binary_accuracy_history = [\n",
    "    np.mean([x[i] for x in all_binary_accuracy_histories]) for i in range(num_epochs)]\n",
    "for i, element in enumerate(average_binary_accuracy_history):\n",
    "    print(i,'-', element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messdaten erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Ein Typ bestimmen !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typNum =''\n",
    "episode_string_mess = \"gesamt_string_mess.csv\"\n",
    "episode_roh_mess = \"gesamt_roh_mess.csv\"\n",
    "episode_binar_mess = \"gesamt_binar_mess.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alte Datei mit gleichem Namen löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(episode_string_mess):\n",
    "    os.remove(episode_string_mess)\n",
    "else:\n",
    "    print(\"Diese Datei existiert nicht\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach den entsprechenden Textdateien im Ordnet suchen und in einer rohe Datei zusammentragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def word_to_lex(word):\n",
    "    ret=(word) \n",
    "    return ret\n",
    "\n",
    "def write_back(words):\n",
    "    with open(episode_roh_mess,\"a\", encoding='utf-8') as output:\n",
    "        for word in words:\n",
    "            #print(word)\n",
    "            as_lex = word_to_lex(word[0])\n",
    "            full_word = '\"' + as_lex + '\"'\n",
    "            for sub_word in word[1:]:\n",
    "                full_word += \" , \"  '\"' + sub_word + '\"'\n",
    "            full_word +=\"\\n\"\n",
    "            output.write(full_word)\n",
    "\n",
    "def clean(line):\n",
    "    line = line.replace(\"\\n\",\" \").strip().lower()\n",
    "    line = line.replace(\"ä\",\"ae\").replace(\"ü\",\"ue\").replace(\"ö\",\"oe\").replace(\"ß\",\"ss\").replace(\",\",\"\").replace(\"«\",\"\")\n",
    "    line = line.replace(\"»\",\"\").replace(\".\",\"\").replace(\":\",\"\").replace(\";\",\"\").replace('\"',\"\")\n",
    "    line = line.replace(\"?\",\"\").replace(\"!\",\"\").replace(\"á\",\"a\").replace(\",\",\"\").replace(\"\\t\",\" \").replace(\"'\",\"\")\n",
    "    line = line.replace(\"‹\",\"\").replace(\"›\",\"\").replace(\"-\",\" \").replace(\"'('\",\"\").replace(\"')'\",\"\").replace('>','')\n",
    "    line = line.replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \").replace('–','').replace('—','').replace('<','')\n",
    "    line = line.replace(\"Â\", \"A\").replace(\"ø\", \"oe\").replace('“','').replace('„','').replace('(','').replace(')','')\n",
    "    line = line.replace(\"‚\", \"\").replace(']','').replace('[','')\n",
    "    if line == \"\": \n",
    "        return\n",
    "    \n",
    "    line=line.split(\"|\")\n",
    "    line[0]=line[0].split(\"|\")[0]\n",
    "    flex=[]\n",
    "    try:\n",
    "        flex=line[1].split(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    value=str(line)\n",
    "    line=str(line)   \n",
    "    flex.append(line)\n",
    "    ret=[]\n",
    "    for i in flex:\n",
    "        ret.append((i,value[0]))\n",
    "    return ret\n",
    "\n",
    "with open(episode_roh_mess, \"w\", encoding='utf-8') as output:\n",
    "    output.write (\"quelle,episode,index_string,index_binar\\n\")\n",
    "pairs = []\n",
    "\n",
    "for file in glob.glob(\"Messdaten/*.txt\"):\n",
    "    if typNum in file:\n",
    "        with open(file, 'r', encoding='utf-8') as episode:\n",
    "            for line in episode.readlines():\n",
    "                clean_words = clean(line)\n",
    "                pairs = pairs + clean_words\n",
    "write_back(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rohdatei endgültig überarbeiten und in eine CSV Datei speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(episode_roh_mess,'r', encoding ='utf-8')\n",
    "fout = open(episode_string_mess, \"wt\", encoding ='utf-8')\n",
    "for kfz in fin:\n",
    "    fout.write(kfz.replace(', \"[\"',\"\").replace('\"[', \"\").replace(']\"',\"\").replace(\"', '\", \"','\").replace(\" '\", \"'\"))                      \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste von einmaligen Episoden erstellen in der CSV Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(episode_string_mess, encoding='utf-8')\n",
    "from collections import Counter\n",
    "indexliste=Counter(df.index_string)\n",
    "print(*indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messdaten laden und aufbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(episode_string_mess,'r', encoding='utf-8') \n",
    "fout = open(episode_binar_mess, \"wt\", encoding='utf-8')\n",
    "for efz in fin:\n",
    "    fout.write(efz.replace(\"'\",\"\"))\n",
    "                                 \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(episode_binar_mess, encoding='utf-8')\n",
    "indexliste=Counter(df.index_string)\n",
    "print(indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(df.index_string):\n",
    "    if e:\n",
    "        df.index_binar[i]='0'\n",
    "    else: \n",
    "        df.index_binar[i]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(episode_binar_mess, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorhandene Zuordnung laden und den verschiedenen Wörtern eindeutige Zahlen zuordnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_episoden_2 = []\n",
    "pbar = pyprind.ProgBar(len(df['episode']),\n",
    "                       title='Episoden Zahlen zuordnen')\n",
    "for episode in df['episode']:\n",
    "    mapped_episoden_2.append([word_to_int.get(word) for word in episode.split()])\n",
    "    pbar.update()\n",
    "\n",
    "mapped_episoden_2 = [list(filter(None, el)) for el in mapped_episoden_2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raiingsndaten und Trainingslabels bestimmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess_data = mapped_episoden_2[0:] \n",
    "mess_labels = df.loc[0:, 'index_binar'].values\n",
    "print(\"sequences =\",max([max(sequences) for sequences in mapped_episoden if len(sequences)>0]),\" \", \n",
    "      \"mess_data =\", len (mess_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messdaten vektorisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=sequences_laenge+1): \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "x_mess = vectorize_sequences(mess_data)\n",
    "y_mess = np.asarray(mess_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messdaten analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(average_binary_accuracy_history) + 1), average_binary_accuracy_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "smooth_binary_accuracy_history = smooth_curve(average_binary_accuracy_history[2:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_binary_accuracy_history) + 1), smooth_binary_accuracy_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modell ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh, compiled model.\n",
    "model = build_model()\n",
    "# Train it on the entirety of the data.\n",
    "model.fit(x_train, y_train,\n",
    "          epochs = 3, batch_size = 1)\n",
    "mess_mse_score, mess_binary_accuracy_score = model.evaluate(x_mess, y_mess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess_binary_accuracy_score, mess_mse_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voraussagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(x_mess)\n",
    "#for i in range(len(predictions)):\n",
    "    #print(model.predict(x_mess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximaler Wert:', max(max(model.predict(x_mess))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voraussgen in den aussortierten Datensätzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_pred = model.predict(x_mess)\n",
    "cutoff_value = 0.8\n",
    "for i in range(len(y_mess)):\n",
    "    if mod_pred[i,0]>cutoff_value:\n",
    "        if ((i==0)|(mod_pred[i-1,0]>cutoff_value))|(mod_pred[i+0]>cutoff_value):\n",
    "            #if df.index_string ==0:\n",
    "                print (i+0,\"-\",\n",
    "                  #i+2,\"-\", \n",
    "                  #y_test[i],\"-\", \n",
    "                  mod_pred[i],\"-\",\n",
    "                  df.quelle[i+0],\"-\", \n",
    "                  df.episode[i+0],\"-\",\n",
    "                  df.index_string[i+0],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voraussage: gewünschte Datensetze abrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_mess)):\n",
    "    if i>30:\n",
    "        print(i+0,\"-\",\n",
    "              model.predict(x_mess)[i],\"-\",\n",
    "              df.quelle[i+0],\"-\",\n",
    "              df.episode[i+0],\"-\",\n",
    "              df.index_string[i+0],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswerter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elementenstruktur definieren: vorherige Episode - gesuchte Episode - folgende Episode\n",
    "\n",
    "!!! Datei definieren !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'a300_string_train.csv' does not exist: b'a300_string_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7ad32e55aab5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a300_string_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mep_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep_full\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'a300_string_train.csv' does not exist: b'a300_string_train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('a300_string_train.csv', encoding='utf-8')\n",
    "from collections import Counter\n",
    "\n",
    "def ep_format(ep_full):\n",
    "    return (ep_full.split('_'))[0]\n",
    "\n",
    "def quellenvergleich (df, i1, i2):\n",
    "#     print(df.quelle[i1],df.quelle[i2])\n",
    "    return df.quelle[i1]==df.quelle[i2]\n",
    "\n",
    "def ast(gesep, df):\n",
    "    ep_tree = {}\n",
    "    a_liste = []\n",
    "    z_liste = []  \n",
    "    df_len = len(df.index_string)\n",
    "    for i, ep in enumerate(df.index_string):\n",
    "        if gesep == ep:\n",
    "             #print(i, ep_full)\n",
    "            if (i > 0)&(quellenvergleich(df, i, i-1)):\n",
    "                a = df.index_string[i-1]\n",
    "            else:\n",
    "                a = 'eAnfang_a_'\n",
    "            if (i < df_len - 1):\n",
    "                if not (quellenvergleich(df, i, i+1)):\n",
    "                    z = 'eEnde_a_'\n",
    "                else:        \n",
    "                    z = df.index_string[i+1]\n",
    "            else:\n",
    "                z = 'eEnde_a_'\n",
    "            a_liste.append(a)\n",
    "            z_liste.append(z)\n",
    "    return {gesep: [Counter(a_liste), Counter(z_liste)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle im Korpus vorhandene Episoden extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alle_aeste(gesep, df):\n",
    "    episoden_baeume = {}\n",
    "    \n",
    "    ep_list = []\n",
    "    for ep_full in df.index_string:  \n",
    "        \n",
    "        ep = ep_format(ep_full)      \n",
    "        if gesep == ep:          \n",
    "            \n",
    "            ep_list.append(ep_full)\n",
    "    for ep in set(ep_list):\n",
    "        episoden_baeume.update(ast(ep,df))\n",
    "    return episoden_baeume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episoden die gesuchte Episode umlegen und in die Liste in die richrige Reihenfolge darstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_val_printer(d):\n",
    "    for k, v in d.items():\n",
    "        print(v, k, sep=':')\n",
    "    #print()\n",
    "        \n",
    "def baum_printer(baeume: dict, baum: str):\n",
    "    key_val_printer (baeume[baum][0])\n",
    "    \n",
    "    \n",
    "    print(80*'-')\n",
    "    print(sum(baeume[baum][0].values()), baum)\n",
    "    print(80*'-')\n",
    "    key_val_printer(baeume[baum][1]) \n",
    "    \n",
    "            \n",
    "def wald_printer(wald: dict):\n",
    "    for baum in sorted(wald.keys()):\n",
    "        baum_printer (wald, baum)\n",
    "        print(80*'=')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Die gesuchte Episode bestimmen !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alle_aeste' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-553be292c3c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msynop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malle_aeste\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'e300\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwald_printer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'alle_aeste' is not defined"
     ]
    }
   ],
   "source": [
    "synop = alle_aeste(\"'e300\", df)\n",
    "wald_printer(synop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
