{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umgebung vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/edadunashvili/ThePrax.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ThePrax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Korpus von Trainingsdaten erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " !!! Den gesuchten Typ bestimmen !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typNum='300'\n",
    "episode_string_train = \"a300_string_train.csv\"\n",
    "episode_roh_train = \"a300_roh_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorhandener gleichnamiger Korpus wird gelöscht "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(episode_string_train):\n",
    "    os.remove(episode_string_train)\n",
    "else:\n",
    "    print(\"Diese Datei existiert nicht\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Ordner \"Trainingsdaten\" nach den entsprechenden Textdateien suchen und in einer rohe Datei zusammentragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def word_to_lex(word):\n",
    "    ret=(word) \n",
    "    return ret\n",
    "\n",
    "def write_back(words):\n",
    "    with open(episode_roh_train,\"a\", encoding='utf-8') as output:\n",
    "        for word in words:\n",
    "            #print(word)\n",
    "            as_lex = word_to_lex(word[0])\n",
    "            full_word = '\"' + as_lex + '\"'\n",
    "            for sub_word in word[1:]:\n",
    "                full_word += \" , \"  '\"' + sub_word + '\"'\n",
    "            full_word +=\"\\n\"\n",
    "            output.write(full_word)\n",
    "\n",
    "def clean(line):\n",
    "    line = line.replace(\"\\n\",\" \").strip().lower()\n",
    "    line = line.replace(\"ä\",\"ae\").replace(\"ü\",\"ue\").replace(\"ö\",\"oe\").replace(\"ß\",\"ss\").replace(\",\",\"\").replace(\"«\",\"\")\n",
    "    line = line.replace(\"»\",\"\").replace(\".\",\"\").replace(\":\",\"\").replace(\";\",\"\").replace('\"',\"\")\n",
    "    line = line.replace(\"?\",\"\").replace(\"!\",\"\").replace(\"á\",\"a\").replace(\",\",\"\").replace(\"\\t\",\" \").replace(\"'\",\"\")\n",
    "    line = line.replace(\"‹\",\"\").replace(\"›\",\"\").replace(\"-\",\" \").replace(\"'('\",\"\").replace(\"')'\",\"\").replace('>','')\n",
    "    line = line.replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \").replace('–','').replace('—','').replace('<','')\n",
    "    line = line.replace(\"Â\", \"A\").replace(\"ø\", \"oe\").replace('“','').replace('„','').replace('(','').replace(')','')\n",
    "    line = line.replace(\"‚\", \"\").replace(']','').replace('[','')\n",
    "    if line == \"\": \n",
    "        return\n",
    "    \n",
    "    line=line.split(\"|\")\n",
    "    line[0]=line[0].split(\"|\")[0]\n",
    "    flex=[]\n",
    "    try:\n",
    "        flex=line[1].split(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    value=str(line)\n",
    "    line=str(line)   \n",
    "    flex.append(line)\n",
    "    ret=[]\n",
    "    for i in flex:\n",
    "        ret.append((i,value[0]))\n",
    "    return ret\n",
    "\n",
    "with open(episode_roh_train, \"w\", encoding='utf-8') as output:\n",
    "    output.write (\"quelle,episode,index_string,index_binar\\n\")\n",
    "pairs = []\n",
    "\n",
    "\n",
    "for file in glob.glob(\"Trainingsdaten/*.txt\"):\n",
    "    \n",
    "    if typNum in file:\n",
    "        with open(file, 'r', encoding='utf-8') as episode:\n",
    "            for line in episode.readlines():\n",
    "                clean_words = clean(line)\n",
    "                pairs = pairs + clean_words\n",
    "write_back(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rohdatei endgültig überarbeiten und in eine CSV Datei speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(episode_roh_train,'r', encoding ='utf-8')\n",
    "fout = open(episode_string_train, \"wt\", encoding ='utf-8')\n",
    "for kfz in fin:\n",
    "    fout.write(kfz.replace(', \"[\"',\"\").replace('\"[', \"\").replace(']\"',\"\").replace(\"', '\", \"','\").replace(\" '\", \"'\"))                      \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste von einmaligen Episoden aus der CSV Datei zeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(episode_string_train, encoding='utf-8')\n",
    "from collections import Counter\n",
    "indexliste=Counter(df.index_string)\n",
    "print(indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell erstellen und trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliotheken laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import models\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Variable anpassen !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ziel_episode = 'e300_c_anfangssituation_ankunft_und_erkundigung_der_not'\n",
    "mini_frequenz = 2\n",
    "k_fach = 5\n",
    "layD = 96\n",
    "episode_string_train = 'a300_string_train.csv' \n",
    "episode_binar_train = 'a300_binar_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufbereitung der Trainingsdaten.\n",
    "Rohdaten laden und und den Episodenbestand betrachten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(episode_string_train,'r', encoding='utf-8') \n",
    "fout = open(episode_binar_train, \"wt\", encoding='utf-8')\n",
    "for efz in fin:\n",
    "    fout.write(efz.replace(\"'\",\"\"))  \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die in der CSV Datei etikettierte Episoden auflisten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(episode_binar_train, encoding='utf-8')\n",
    "indexliste=Counter(df.index_string)\n",
    "print(*indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die gesuchte Episode etikettieren und den Rohdaten in Trainingsdaten umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(df.index_string):\n",
    "    if e == ziel_episode:\n",
    "        df.index_binar[i]='1'\n",
    "    else: \n",
    "        df.index_binar[i]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(episode_binar_train, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainngsdaten aufbereiten. Das Vorkommen jedes Wortes zählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "w = stopwords.words('german')\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['episode']),\n",
    "                       title='Vorkommen der Wörter zählen')\n",
    "for i, episode in enumerate(df['episode']): \n",
    "        pbar.update()\n",
    "        counts.update(episode.split())\n",
    "new_counts = {}\n",
    "for k, v in counts.items():\n",
    "    if v > mini_frequenz and k not in w:\n",
    "        new_counts[k] = v\n",
    "counts = Counter(new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuordnung erzeugen und den verschiedenen Wörtern eindeutige Zahlen zuordnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts,0)}\n",
    "mapped_episoden = []\n",
    "pbar = pyprind.ProgBar(len(df['episode']),\n",
    "                       title='Episoden Zahlen zuordnen')\n",
    "for episode in df['episode']:\n",
    "    mapped_episoden.append([word_to_int.get(word) for word in episode.split()])\n",
    "    pbar.update()\n",
    "mapped_episoden = [list(filter(None, el)) for el in mapped_episoden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(word_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mapped_episoden[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traiingsndaten und Trainingslabels bestimmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mapped_episoden[0:] \n",
    "train_labels = df.loc[0:, 'index_binar'].values\n",
    "print(\"sequences =\",max([max(sequences) for sequences in mapped_episoden if len(sequences)>0]),\" \",\n",
    "      \"train_data =\", len (train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainingsdaten und -Labels vektorisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_laenge=max([max(sequences) for sequences in mapped_episoden if len(sequences)>0])\n",
    "def vectorize_sequences(sequences, dimension=sequences_laenge+1): \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "x_train = vectorize_sequences(train_data) \n",
    "y_train = np.asarray(train_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, element in enumerate(x_train[0]):\n",
    "    #print(i,'-', element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuronales Netz erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(layD, activation='tanh',\n",
    "                           input_shape=(sequences_laenge+1,)))\n",
    "    #model.add(layers.Dense(layD, activation='relu'))\n",
    "    model.add(layers.Dense(layD, activation='tanh')) \n",
    "    model.add(layers.Dense((1), activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuronales Netz trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = k_fach\n",
    "num_val_samples = len(x_train) // k \n",
    "num_epochs = 4\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_x_train = np.concatenate(\n",
    "        [x_train[:i * num_val_samples],\n",
    "         x_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_y_train = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    model.fit(partial_x_train, partial_y_train,\n",
    "              epochs=num_epochs, batch_size = 8, verbose=0)\n",
    "    # Evaluate the model on the validation data\n",
    "    val_mse, val_binary_accuracy = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_binary_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 8\n",
    "all_binary_accuracy_histories = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_x_train = np.concatenate(\n",
    "        [x_train[:i * num_val_samples],\n",
    "         x_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_y_train = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    history = model.fit(partial_x_train, partial_y_train,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size = 8, verbose=0)\n",
    "    binary_accuracy_history = history.history['val_binary_accuracy']\n",
    "    all_binary_accuracy_histories.append(binary_accuracy_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_binary_accuracy_history = [\n",
    "    np.mean([x[i] for x in all_binary_accuracy_histories]) for i in range(num_epochs)]\n",
    "for i, element in enumerate(average_binary_accuracy_history):\n",
    "    print(i,'-', element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messdaten erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Den gesuchten Typ bestimmen !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typNum =''\n",
    "episode_string_mess = \"gesamt_string_mess.csv\"\n",
    "episode_roh_mess = \"gesamt_roh_mess.csv\"\n",
    "episode_binar_mess = \"gesamt_binar_mess.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alte Datei mit gleichem Namen löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(episode_string_mess):\n",
    "    os.remove(episode_string_mess)\n",
    "else:\n",
    "    print(\"Diese Datei existiert nicht\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach den entsprechenden Textdateien im Ordnet suchen und in einer rohe Datei zusammentragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def word_to_lex(word):\n",
    "    ret=(word) \n",
    "    return ret\n",
    "\n",
    "def write_back(words):\n",
    "    with open(episode_roh_mess,\"a\", encoding='utf-8') as output:\n",
    "        for word in words:\n",
    "            #print(word)\n",
    "            as_lex = word_to_lex(word[0])\n",
    "            full_word = '\"' + as_lex + '\"'\n",
    "            for sub_word in word[1:]:\n",
    "                full_word += \" , \"  '\"' + sub_word + '\"'\n",
    "            full_word +=\"\\n\"\n",
    "            output.write(full_word)\n",
    "\n",
    "def clean(line):\n",
    "    line = line.replace(\"\\n\",\" \").strip().lower()\n",
    "    line = line.replace(\"ä\",\"ae\").replace(\"ü\",\"ue\").replace(\"ö\",\"oe\").replace(\"ß\",\"ss\").replace(\",\",\"\").replace(\"«\",\"\")\n",
    "    line = line.replace(\"»\",\"\").replace(\".\",\"\").replace(\":\",\"\").replace(\";\",\"\").replace('\"',\"\")\n",
    "    line = line.replace(\"?\",\"\").replace(\"!\",\"\").replace(\"á\",\"a\").replace(\",\",\"\").replace(\"\\t\",\" \").replace(\"'\",\"\")\n",
    "    line = line.replace(\"‹\",\"\").replace(\"›\",\"\").replace(\"-\",\" \").replace(\"'('\",\"\").replace(\"')'\",\"\").replace('>','')\n",
    "    line = line.replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \").replace('–','').replace('—','').replace('<','')\n",
    "    line = line.replace(\"Â\", \"A\").replace(\"ø\", \"oe\").replace('“','').replace('„','').replace('(','').replace(')','')\n",
    "    line = line.replace(\"‚\", \"\").replace(']','').replace('[','')\n",
    "    if line == \"\": \n",
    "        return\n",
    "    \n",
    "    line=line.split(\"|\")\n",
    "    line[0]=line[0].split(\"|\")[0]\n",
    "    flex=[]\n",
    "    try:\n",
    "        flex=line[1].split(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    value=str(line)\n",
    "    line=str(line)   \n",
    "    flex.append(line)\n",
    "    ret=[]\n",
    "    for i in flex:\n",
    "        ret.append((i,value[0]))\n",
    "    return ret\n",
    "\n",
    "with open(episode_roh_mess, \"w\", encoding='utf-8') as output:\n",
    "    output.write (\"quelle,episode,index_string,index_binar\\n\")\n",
    "pairs = []\n",
    "\n",
    "for file in glob.glob(\"Messdaten/*.txt\"):\n",
    "    if typNum in file:\n",
    "        with open(file, 'r', encoding='utf-8') as episode:\n",
    "            for line in episode.readlines():\n",
    "                clean_words = clean(line)\n",
    "                pairs = pairs + clean_words\n",
    "write_back(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rohdatei endgültig überarbeiten und in eine CSV Datei speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(episode_roh_mess,'r', encoding ='utf-8')\n",
    "fout = open(episode_string_mess, \"wt\", encoding ='utf-8')\n",
    "for kfz in fin:\n",
    "    fout.write(kfz.replace(', \"[\"',\"\").replace('\"[', \"\").replace(']\"',\"\").replace(\"', '\", \"','\").replace(\" '\", \"'\"))                      \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die in der CSV Datei etikettierte Episoden auflisten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(episode_string_mess, encoding='utf-8')\n",
    "from collections import Counter\n",
    "indexliste=Counter(df.index_string)\n",
    "print(*indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messdaten laden und aufbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(episode_string_mess,'r', encoding='utf-8') \n",
    "fout = open(episode_binar_mess, \"wt\", encoding='utf-8')\n",
    "for efz in fin:\n",
    "    fout.write(efz.replace(\"'\",\"\"))\n",
    "                                 \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(episode_binar_mess, encoding='utf-8')\n",
    "indexliste=Counter(df.index_string)\n",
    "print(indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(df.index_string):\n",
    "    if e:\n",
    "        df.index_binar[i]='0'\n",
    "    else: \n",
    "        df.index_binar[i]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(episode_binar_mess, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorhandene Zuordnung laden und den verschiedenen Wörtern eindeutige Zahlen zuordnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_episoden_2 = []\n",
    "pbar = pyprind.ProgBar(len(df['episode']),\n",
    "                       title='Episoden Zahlen zuordnen')\n",
    "for episode in df['episode']:\n",
    "    mapped_episoden_2.append([word_to_int.get(word) for word in episode.split()])\n",
    "    pbar.update()\n",
    "\n",
    "mapped_episoden_2 = [list(filter(None, el)) for el in mapped_episoden_2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raiingsndaten und Trainingslabels bestimmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess_data = mapped_episoden_2[0:] \n",
    "mess_labels = df.loc[0:, 'index_binar'].values\n",
    "print(\"sequences =\",max([max(sequences) for sequences in mapped_episoden if len(sequences)>0]),\" \", \n",
    "      \"mess_data =\", len (mess_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messdaten vektorisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=sequences_laenge+1): \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "x_mess = vectorize_sequences(mess_data)\n",
    "y_mess = np.asarray(mess_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messdaten analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(average_binary_accuracy_history) + 1), average_binary_accuracy_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!! Zahl epochs, wenn nötig, in dem 11. Satz anpassen !!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "smooth_binary_accuracy_history = smooth_curve(average_binary_accuracy_history[2:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_binary_accuracy_history) + 1), smooth_binary_accuracy_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modell ausführen. !!!!!! wenn nötig, Zahl der epochs im fünften Satz korrigieren !!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh, compiled model.\n",
    "model = build_model()\n",
    "# Train it on the entirety of the data.\n",
    "model.fit(x_train, y_train,\n",
    "          epochs = 3, batch_size = 1)\n",
    "mess_mse_score, mess_binary_accuracy_score = model.evaluate(x_mess, y_mess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess_binary_accuracy_score, mess_mse_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voraussagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(x_mess)\n",
    "#for i in range(len(predictions)):\n",
    "    #print(model.predict(x_mess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximaler Wert:', max(max(model.predict(x_mess))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voraussgen in den aussortierten Datensätzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_pred = model.predict(x_mess)\n",
    "cutoff_value = 0.3\n",
    "for i in range(len(y_mess)):\n",
    "    if mod_pred[i,0]>cutoff_value:\n",
    "        if ((i==0)|(mod_pred[i-1,0]>cutoff_value))|(mod_pred[i+0]>cutoff_value):\n",
    "            #if df.index_string ==0:\n",
    "                print (i+0,\"-\",\n",
    "                  #i+2,\"-\", \n",
    "                  #y_test[i],\"-\", \n",
    "                  mod_pred[i],\"-\",\n",
    "                  df.quelle[i+0],\"-\", \n",
    "                  df.episode[i+0],\"-\",\n",
    "                  df.index_string[i+0],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voraussage: gewünschte Datensetze abrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_mess)):\n",
    "    if i>30:\n",
    "        print(i+0,\"-\",\n",
    "              model.predict(x_mess)[i],\"-\",\n",
    "              df.quelle[i+0],\"-\",\n",
    "              df.episode[i+0],\"-\",\n",
    "              df.index_string[i+0],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
