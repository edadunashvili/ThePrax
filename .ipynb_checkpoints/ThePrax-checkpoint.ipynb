{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umgebung vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/edadunashvili/ThePrax.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ThePrax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testdatencorpushersteller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " !!! Ein Typ bestimmen !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typNum='300'\n",
    "episode_string_train = \"a300_string_train.csv\"\n",
    "episode_roh_train = \"a300_roh_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorhandener gleichnamiger Korpus wird gelöscht "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(episode_string_train):\n",
    "    os.remove(episode_string_train)\n",
    "else:\n",
    "    print(\"Diese Datei existiert nicht\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Ordner \"Trainingsdaten\" nach den entsprechenden Textdateien suchen und in einer rohe Datei zusammentragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def word_to_lex(word):\n",
    "    ret=(word) \n",
    "    return ret\n",
    "\n",
    "def write_back(words):\n",
    "    with open(episode_roh_train,\"a\", encoding='utf-8') as output:\n",
    "        for word in words:\n",
    "            #print(word)\n",
    "            as_lex = word_to_lex(word[0])\n",
    "            full_word = '\"' + as_lex + '\"'\n",
    "            for sub_word in word[1:]:\n",
    "                full_word += \" , \"  '\"' + sub_word + '\"'\n",
    "            full_word +=\"\\n\"\n",
    "            output.write(full_word)\n",
    "\n",
    "def clean(line):\n",
    "    line = line.replace(\"\\n\",\" \").strip().lower()\n",
    "    line = line.replace(\"ä\",\"ae\").replace(\"ü\",\"ue\").replace(\"ö\",\"oe\").replace(\"ß\",\"ss\").replace(\",\",\"\").replace(\"«\",\"\")\n",
    "    line = line.replace(\"»\",\"\").replace(\".\",\"\").replace(\":\",\"\").replace(\";\",\"\").replace('\"',\"\")\n",
    "    line = line.replace(\"?\",\"\").replace(\"!\",\"\").replace(\"á\",\"a\").replace(\",\",\"\").replace(\"\\t\",\" \").replace(\"'\",\"\")\n",
    "    line = line.replace(\"‹\",\"\").replace(\"›\",\"\").replace(\"-\",\" \").replace(\"'('\",\"\").replace(\"')'\",\"\").replace('>','')\n",
    "    line = line.replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \").replace('–','').replace('—','').replace('<','')\n",
    "    line = line.replace(\"Â\", \"A\").replace(\"ø\", \"oe\").replace('“','').replace('„','').replace('(','').replace(')','')\n",
    "    line = line.replace(\"‚\", \"\").replace(']','').replace('[','')\n",
    "    if line == \"\": \n",
    "        return\n",
    "    \n",
    "    line=line.split(\"|\")\n",
    "    line[0]=line[0].split(\"|\")[0]\n",
    "    flex=[]\n",
    "    try:\n",
    "        flex=line[1].split(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    value=str(line)\n",
    "    line=str(line)   \n",
    "    flex.append(line)\n",
    "    ret=[]\n",
    "    for i in flex:\n",
    "        ret.append((i,value[0]))\n",
    "    return ret\n",
    "\n",
    "with open(episode_roh_train, \"w\", encoding='utf-8') as output:\n",
    "    output.write (\"quelle,episode,index_string,index_binar\\n\")\n",
    "pairs = []\n",
    "\n",
    "\n",
    "for file in glob.glob(\"Trainingsdaten/*.txt\"):\n",
    "    \n",
    "    if typNum in file:\n",
    "        with open(file, 'r', encoding='utf-8') as episode:\n",
    "            for line in episode.readlines():\n",
    "                clean_words = clean(line)\n",
    "                pairs = pairs + clean_words\n",
    "write_back(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rohdatei endgültig überarbeiten und in eine CSV Datei speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(episode_roh_train,'r', encoding ='utf-8')\n",
    "fout = open(episode_string_train, \"wt\", encoding ='utf-8')\n",
    "for kfz in fin:\n",
    "    fout.write(kfz.replace(', \"[\"',\"\").replace('\"[', \"\").replace(']\"',\"\").replace(\"', '\", \"','\").replace(\" '\", \"'\"))                      \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste von einmaligen Episoden aus der CSV Datei zeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(episode_string_train, encoding='utf-8')\n",
    "from collections import Counter\n",
    "indexliste=Counter(df.index_string)\n",
    "print(*indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswerter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elementenstruktur definieren: vorherige Episode - gesuchte Episode - folgende Episode\n",
    "\n",
    "!!! Datei definieren !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('a300_string_train.csv', encoding='utf-8')\n",
    "from collections import Counter\n",
    "\n",
    "def ep_format(ep_full):\n",
    "    return (ep_full.split('_'))[0]\n",
    "\n",
    "def quellenvergleich (df, i1, i2):\n",
    "#     print(df.quelle[i1],df.quelle[i2])\n",
    "    return df.quelle[i1]==df.quelle[i2]\n",
    "\n",
    "def ast(gesep, df):\n",
    "    ep_tree = {}\n",
    "    a_liste = []\n",
    "    z_liste = []  \n",
    "    df_len = len(df.index_string)\n",
    "    for i, ep in enumerate(df.index_string):\n",
    "        if gesep == ep:\n",
    "             #print(i, ep_full)\n",
    "            if (i > 0)&(quellenvergleich(df, i, i-1)):\n",
    "                a = df.index_string[i-1]\n",
    "            else:\n",
    "                a = 'eAnfang_a_'\n",
    "            if (i < df_len - 1):\n",
    "                if not (quellenvergleich(df, i, i+1)):\n",
    "                    z = 'eEnde_a_'\n",
    "                else:        \n",
    "                    z = df.index_string[i+1]\n",
    "            else:\n",
    "                z = 'eEnde_a_'\n",
    "            a_liste.append(a)\n",
    "            z_liste.append(z)\n",
    "    return {gesep: [Counter(a_liste), Counter(z_liste)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle im Korpus vorhandene Episoden extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alle_aeste(gesep, df):\n",
    "    episoden_baeume = {}\n",
    "    \n",
    "    ep_list = []\n",
    "    for ep_full in df.index_string:  \n",
    "        \n",
    "        ep = ep_format(ep_full)      \n",
    "        if gesep == ep:          \n",
    "            \n",
    "            ep_list.append(ep_full)\n",
    "    for ep in set(ep_list):\n",
    "        episoden_baeume.update(ast(ep,df))\n",
    "    return episoden_baeume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episoden die gesuchte Episode umlegen und in die Liste in die richrige Reihenfolge darstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_val_printer(d):\n",
    "    for k, v in d.items():\n",
    "        print(v, k, sep=':')\n",
    "    #print()\n",
    "        \n",
    "def baum_printer(baeume: dict, baum: str):\n",
    "    key_val_printer (baeume[baum][0])\n",
    "    \n",
    "    \n",
    "    print(80*'-')\n",
    "    print(sum(baeume[baum][0].values()), baum)\n",
    "    print(80*'-')\n",
    "    key_val_printer(baeume[baum][1]) \n",
    "    \n",
    "            \n",
    "def wald_printer(wald: dict):\n",
    "    for baum in sorted(wald.keys()):\n",
    "        baum_printer (wald, baum)\n",
    "        print(80*'=')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Die gesuchte Episode bestimmen !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synop = alle_aeste(\"'e300\", df)\n",
    "wald_printer(synop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell erstellen und trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import models\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Variable anpassen !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ziel_episode = 'e300_e_vorfeld_des_kampfes_bis_zum_sieg'\n",
    "mini_frequenz = 2\n",
    "k_fach = 5\n",
    "layD = 96\n",
    "episode_string_train = 'a300_string_train.csv' \n",
    "episode_binar_train = 'a300_binar_train.csv'\n",
    "episode_string_mess = 'gesamt_string_mess.csv'\n",
    "episode_binar_mess = 'gesamt_binar_mess.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufbereitung der Trainingsdaten\n",
    "Rohdaten laden und und den Episodenbestand betrachten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(episode_string_train,'r', encoding='utf-8') \n",
    "fout = open(episode_binar_train, \"wt\", encoding='utf-8')\n",
    "for efz in fin:\n",
    "    fout.write(efz.replace(\"'\",\"\"))  \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(episode_binar_train, encoding='utf-8')\n",
    "indexliste=Counter(df.index_string)\n",
    "print(*indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die gesuchte Episode etikettieren und den Rohdaten in Trainingsdaten umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(df.index_string):\n",
    "    if e == ziel_episode:\n",
    "        df.index_binar[i]='1'\n",
    "    else: \n",
    "        df.index_binar[i]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(episode_binar_train, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainngsdaten aufbereiten\n",
    "4.1. Das Vorkommen jedes Wortes zählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "w = stopwords.words('german')\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['episode']),\n",
    "                       title='Vorkommen der Wörter zählen')\n",
    "for i, episode in enumerate(df['episode']): \n",
    "        pbar.update()\n",
    "        counts.update(episode.split())\n",
    "new_counts = {}\n",
    "for k, v in counts.items():\n",
    "    if v > mini_frequenz and k not in w:\n",
    "        new_counts[k] = v\n",
    "counts = Counter(new_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuordnung erzeugen und den verschiedenen Wörtern eindeutige Zahlen zuordnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts,0)}\n",
    "mapped_episoden = []\n",
    "pbar = pyprind.ProgBar(len(df['episode']),\n",
    "                       title='Episoden Zahlen zuordnen')\n",
    "for episode in df['episode']:\n",
    "    mapped_episoden.append([word_to_int.get(word) for word in episode.split()])\n",
    "    pbar.update()\n",
    "mapped_episoden = [list(filter(None, el)) for el in mapped_episoden]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traiingsndaten und Trainingslabels bestimmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mapped_episoden[0:] \n",
    "train_labels = df.loc[0:, 'index_binar'].values\n",
    "print(\"sequences =\",max([max(sequences) for sequences in mapped_episoden if len(sequences)>0]),\" \",\n",
    "      \"train_data =\", len (train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainingsdaten und -Labels vektorisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_laenge=max([max(sequences) for sequences in mapped_episoden if len(sequences)>0])\n",
    "def vectorize_sequences(sequences, dimension=sequences_laenge+1): \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "x_train = vectorize_sequences(train_data) \n",
    "y_train = np.asarray(train_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, element in enumerate(x_train[63]):\n",
    "    #print(i,'-', element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuronales Netz erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(layD, activation='tanh',\n",
    "                           input_shape=(sequences_laenge+1,)))\n",
    "    #model.add(layers.Dense(layD, activation='relu'))\n",
    "    model.add(layers.Dense(layD, activation='tanh')) \n",
    "    model.add(layers.Dense((1), activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuronales Netz trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = k_fach\n",
    "num_val_samples = len(x_train) // k \n",
    "num_epochs = 4\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_x_train = np.concatenate(\n",
    "        [x_train[:i * num_val_samples],\n",
    "         x_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_y_train = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    model.fit(partial_x_train, partial_y_train,\n",
    "              epochs=num_epochs, batch_size = 8, verbose=0)\n",
    "    # Evaluate the model on the validation data\n",
    "    val_mse, val_binary_accuracy = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_binary_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 8\n",
    "all_binary_accuracy_histories = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_x_train = np.concatenate(\n",
    "        [x_train[:i * num_val_samples],\n",
    "         x_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_y_train = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    history = model.fit(partial_x_train, partial_y_train,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size = 8, verbose=0)\n",
    "    binary_accuracy_history = history.history['val_binary_accuracy']\n",
    "    all_binary_accuracy_histories.append(binary_accuracy_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_binary_accuracy_history = [\n",
    "    np.mean([x[i] for x in all_binary_accuracy_histories]) for i in range(num_epochs)]\n",
    "for i, element in enumerate(average_binary_accuracy_history):\n",
    "    print(i,'-', element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messdaten erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Ein Typ bestimmen !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typNum =''\n",
    "episode_string_train = \"gesamt_string_mess.csv\"\n",
    "episode_roh_train = \"gesamt_roh_mess.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alte Datei mit gleichem Namen löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(episode_string_train):\n",
    "    os.remove(episode_string_train)\n",
    "else:\n",
    "    print(\"Diese Datei existiert nicht\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach den entsprechenden Textdateien im Ordnet suchen und in einer rohe Datei zusammentragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def word_to_lex(word):\n",
    "    ret=(word) \n",
    "    return ret\n",
    "\n",
    "def write_back(words):\n",
    "    with open(episode_roh_train,\"a\", encoding='utf-8') as output:\n",
    "        for word in words:\n",
    "            #print(word)\n",
    "            as_lex = word_to_lex(word[0])\n",
    "            full_word = '\"' + as_lex + '\"'\n",
    "            for sub_word in word[1:]:\n",
    "                full_word += \" , \"  '\"' + sub_word + '\"'\n",
    "            full_word +=\"\\n\"\n",
    "            output.write(full_word)\n",
    "\n",
    "def clean(line):\n",
    "    line = line.replace(\"\\n\",\" \").strip().lower()\n",
    "    line = line.replace(\"ä\",\"ae\").replace(\"ü\",\"ue\").replace(\"ö\",\"oe\").replace(\"ß\",\"ss\").replace(\",\",\"\").replace(\"«\",\"\")\n",
    "    line = line.replace(\"»\",\"\").replace(\".\",\"\").replace(\":\",\"\").replace(\";\",\"\").replace('\"',\"\")\n",
    "    line = line.replace(\"?\",\"\").replace(\"!\",\"\").replace(\"á\",\"a\").replace(\",\",\"\").replace(\"\\t\",\" \").replace(\"'\",\"\")\n",
    "    line = line.replace(\"‹\",\"\").replace(\"›\",\"\").replace(\"-\",\" \").replace(\"'('\",\"\").replace(\"')'\",\"\").replace('>','')\n",
    "    line = line.replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \").replace('–','').replace('—','').replace('<','')\n",
    "    line = line.replace(\"Â\", \"A\").replace(\"ø\", \"oe\").replace('“','').replace('„','').replace('(','').replace(')','')\n",
    "    line = line.replace(\"‚\", \"\").replace(']','').replace('[','')\n",
    "    if line == \"\": \n",
    "        return\n",
    "    \n",
    "    line=line.split(\"|\")\n",
    "    line[0]=line[0].split(\"|\")[0]\n",
    "    flex=[]\n",
    "    try:\n",
    "        flex=line[1].split(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    value=str(line)\n",
    "    line=str(line)   \n",
    "    flex.append(line)\n",
    "    ret=[]\n",
    "    for i in flex:\n",
    "        ret.append((i,value[0]))\n",
    "    return ret\n",
    "\n",
    "with open(episode_roh_train, \"w\", encoding='utf-8') as output:\n",
    "    output.write (\"quelle,episode,index_string,index_binar\\n\")\n",
    "pairs = []\n",
    "\n",
    "for file in glob.glob(\"Messdaten/*.txt\"):\n",
    "    if typNum in file:\n",
    "        with open(file, 'r', encoding='utf-8') as episode:\n",
    "            for line in episode.readlines():\n",
    "                clean_words = clean(line)\n",
    "                pairs = pairs + clean_words\n",
    "write_back(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rohdatei endgültig überarbeiten und in eine CSV Datei speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(episode_roh_train,'r', encoding ='utf-8')\n",
    "fout = open(episode_string_train, \"wt\", encoding ='utf-8')\n",
    "for kfz in fin:\n",
    "    fout.write(kfz.replace(', \"[\"',\"\").replace('\"[', \"\").replace(']\"',\"\").replace(\"', '\", \"','\").replace(\" '\", \"'\"))                      \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste von einmaligen Episoden erstellen in der CSV Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(episode_string_train, encoding='utf-8')\n",
    "from collections import Counter\n",
    "indexliste=Counter(df.index_string)\n",
    "print(*indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messdaten laden und aufbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(episode_string_mess,'r', encoding='utf-8') \n",
    "fout = open(episode_binar_mess, \"wt\", encoding='utf-8')\n",
    "for efz in fin:\n",
    "    fout.write(efz.replace(\"'\",\"\"))\n",
    "                                 \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(episode_binar_mess, encoding='utf-8')\n",
    "indexliste=Counter(df.index_string)\n",
    "print(indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(df.index_string):\n",
    "    if e:\n",
    "        df.index_binar[i]='0'\n",
    "    else: \n",
    "        df.index_binar[i]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(episode_binar_mess, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorhandene Zuordnung laden und den verschiedenen Wörtern eindeutige Zahlen zuordnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_episoden_2 = []\n",
    "pbar = pyprind.ProgBar(len(df['episode']),\n",
    "                       title='Episoden Zahlen zuordnen')\n",
    "for episode in df['episode']:\n",
    "    mapped_episoden_2.append([word_to_int.get(word) for word in episode.split()])\n",
    "    pbar.update()\n",
    "\n",
    "mapped_episoden_2 = [list(filter(None, el)) for el in mapped_episoden_2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raiingsndaten und Trainingslabels bestimmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess_data = mapped_episoden_2[0:] \n",
    "mess_labels = df.loc[0:, 'index_binar'].values\n",
    "print(\"sequences =\",max([max(sequences) for sequences in mapped_episoden if len(sequences)>0]),\" \", \n",
    "      \"mess_data =\", len (mess_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messdaten vektorisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=sequences_laenge+1): \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "x_mess = vectorize_sequences(mess_data)\n",
    "y_mess = np.asarray(mess_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messdaten analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(average_binary_accuracy_history) + 1), average_binary_accuracy_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "smooth_binary_accuracy_history = smooth_curve(average_binary_accuracy_history[3:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_binary_accuracy_history) + 1), smooth_binary_accuracy_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modell ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh, compiled model.\n",
    "model = build_model()\n",
    "# Train it on the entirety of the data.\n",
    "model.fit(x_train, y_train,\n",
    "          epochs = 2, batch_size = 1)\n",
    "mess_mse_score, mess_binary_accuracy_score = model.evaluate(x_mess, y_mess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess_binary_accuracy_score, mess_mse_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voraussagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(x_mess)\n",
    "#for i in range(len(predictions)):\n",
    "    #print(model.predict(x_mess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximaler Wert:', max(max(model.predict(x_mess))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voraussgen in den aussortierten Datensätzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_pred = model.predict(x_mess)\n",
    "cutoff_value = 0.1\n",
    "for i in range(len(y_mess)):\n",
    "    if mod_pred[i,0]>cutoff_value:\n",
    "        if ((i==0)|(mod_pred[i-1,0]>cutoff_value))|(mod_pred[i+0]>cutoff_value):\n",
    "            #if df.index_string ==0:\n",
    "                print (i+0,\"-\",\n",
    "                  #i+2,\"-\", \n",
    "                  #y_test[i],\"-\", \n",
    "                  mod_pred[i],\"-\",\n",
    "                  df.quelle[i+0],\"-\", \n",
    "                  df.episode[i+0],\"-\",\n",
    "                  df.index_string[i+0],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voraussage: gewünschte Datensetze abrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_mess)):\n",
    "    if i>3 and i<14:\n",
    "        print(i+0,\"-\",\n",
    "              model.predict(x_mess)[i],\"-\",\n",
    "              df.quelle[i+0],\"-\",\n",
    "              df.episode[i+0],\"-\",\n",
    "              df.index_string[i+0],'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
