{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umgebung vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/edadunashvili/ThePrax.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ThePrax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Korpus von etikettierten Daten erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " !!! Den gesuchten Typ bestimmen !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typNum='300'\n",
    "episode_string_train = \"a300_string_train.csv\"\n",
    "episode_roh_train = \"a300_roh_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorhandener gleichnamiger Korpus wird gelöscht "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(episode_string_train):\n",
    "    os.remove(episode_string_train)\n",
    "else:\n",
    "    print(\"Diese Datei existiert nicht\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Ordner \"Trainingsdaten\" nach den entsprechenden Textdateien suchen und in einer rohe Datei zusammentragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def word_to_lex(word):\n",
    "    ret=(word) \n",
    "    return ret\n",
    "\n",
    "def write_back(words):\n",
    "    with open(episode_roh_train,\"a\", encoding='utf-8') as output:\n",
    "        for word in words:\n",
    "            #print(word)\n",
    "            as_lex = word_to_lex(word[0])\n",
    "            full_word = '\"' + as_lex + '\"'\n",
    "            for sub_word in word[1:]:\n",
    "                full_word += \" , \"  '\"' + sub_word + '\"'\n",
    "            full_word +=\"\\n\"\n",
    "            output.write(full_word)\n",
    "\n",
    "def clean(line):\n",
    "    line = line.replace(\"\\n\",\" \").strip().lower()\n",
    "    line = line.replace(\"ä\",\"ae\").replace(\"ü\",\"ue\").replace(\"ö\",\"oe\").replace(\"ß\",\"ss\").replace(\",\",\"\").replace(\"«\",\"\")\n",
    "    line = line.replace(\"»\",\"\").replace(\".\",\"\").replace(\":\",\"\").replace(\";\",\"\").replace('\"',\"\")\n",
    "    line = line.replace(\"?\",\"\").replace(\"!\",\"\").replace(\"á\",\"a\").replace(\",\",\"\").replace(\"\\t\",\" \").replace(\"'\",\"\")\n",
    "    line = line.replace(\"‹\",\"\").replace(\"›\",\"\").replace(\"-\",\" \").replace(\"'('\",\"\").replace(\"')'\",\"\").replace('>','')\n",
    "    line = line.replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \").replace('–','').replace('—','').replace('<','')\n",
    "    line = line.replace(\"Â\", \"A\").replace(\"ø\", \"oe\").replace('“','').replace('„','').replace('(','').replace(')','')\n",
    "    line = line.replace(\"‚\", \"\").replace(']','').replace('[','')\n",
    "    if line == \"\": \n",
    "        return\n",
    "    \n",
    "    line=line.split(\"|\")\n",
    "    line[0]=line[0].split(\"|\")[0]\n",
    "    flex=[]\n",
    "    try:\n",
    "        flex=line[1].split(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    value=str(line)\n",
    "    line=str(line)   \n",
    "    flex.append(line)\n",
    "    ret=[]\n",
    "    for i in flex:\n",
    "        ret.append((i,value[0]))\n",
    "    return ret\n",
    "\n",
    "with open(episode_roh_train, \"w\", encoding='utf-8') as output:\n",
    "    output.write (\"quelle,episode,index_string,index_binar\\n\")\n",
    "pairs = []\n",
    "\n",
    "\n",
    "for file in glob.glob(\"Trainingsdaten/*.txt\"):\n",
    "    \n",
    "    if typNum in file:\n",
    "        with open(file, 'r', encoding='utf-8') as episode:\n",
    "            for line in episode.readlines():\n",
    "                clean_words = clean(line)\n",
    "                pairs = pairs + clean_words\n",
    "write_back(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rohdatei endgültig überarbeiten und in eine CSV Datei speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open(episode_roh_train,'r', encoding ='utf-8')\n",
    "fout = open(episode_string_train, \"wt\", encoding ='utf-8')\n",
    "for kfz in fin:\n",
    "    fout.write(kfz.replace(', \"[\"',\"\").replace('\"[', \"\").replace(']\"',\"\").replace(\"', '\", \"','\").replace(\" '\", \"'\"))                      \n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste von einmaligen Episoden aus der CSV Datei zeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(episode_string_train, encoding='utf-8')\n",
    "from collections import Counter\n",
    "indexliste=Counter(df.index_string)\n",
    "print(*indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswerter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elementenstruktur definieren: vorherige Episode - gesuchte Episode - folgende Episode\n",
    "\n",
    "!!! Datei definieren !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('a300_string_train.csv', encoding='utf-8')\n",
    "from collections import Counter\n",
    "\n",
    "def ep_format(ep_full):\n",
    "    return (ep_full.split('_'))[0]\n",
    "\n",
    "def ep_name_format(ep_full):\n",
    "    sublist = (ep_full.split('_'))[0:2]\n",
    "    name = \"\"\n",
    "    for strg in sublist:\n",
    "        name += strg+'_'\n",
    "    return name\n",
    "\n",
    "def ep_name_vollformat(ep_full):\n",
    "    sublist = (ep_full.split())\n",
    "    vollname = \"\"\n",
    "    for strg in sublist:\n",
    "        vollname += strg\n",
    "    return vollname\n",
    "\n",
    "def quellenvergleich (df, i1, i2):\n",
    "#     print(df.quelle[i1],df.quelle[i2])\n",
    "    return df.quelle[i1]==df.quelle[i2]\n",
    "\n",
    "def ast(gesep, df):\n",
    "    ep_tree = {}\n",
    "    a_liste = []\n",
    "    z_liste = []  \n",
    "    df_len = len(df.index_string)\n",
    "    for i, ep in enumerate(df.index_string):\n",
    "        if gesep == ep:\n",
    "             #print(i, ep_full)\n",
    "            if (i > 0)&(quellenvergleich(df, i, i-1)):\n",
    "                a = df.index_string[i-1]\n",
    "            else:\n",
    "                a = 'eAnfang_a_'\n",
    "            if (i < df_len - 1):\n",
    "                if not (quellenvergleich(df, i, i+1)):\n",
    "                    z = 'eEnde_a_'\n",
    "                else:        \n",
    "                    z = df.index_string[i+1]\n",
    "            else:\n",
    "                z = 'eEnde_a_'\n",
    "            a_liste.append(a)\n",
    "            z_liste.append(z)\n",
    "    return {gesep: [Counter(a_liste), Counter(z_liste)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle im Korpus vorhandene Episoden extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alle_aeste(gesep, df):\n",
    "    episoden_baeume = {}\n",
    "    \n",
    "    ep_list = []\n",
    "    for ep_full in df.index_string:  \n",
    "        \n",
    "        ep = ep_format(ep_full)      \n",
    "        if gesep == ep:          \n",
    "            \n",
    "            ep_list.append(ep_full)\n",
    "    for ep in set(ep_list):\n",
    "        episoden_baeume.update(ast(ep,df))\n",
    "    return episoden_baeume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episoden die gesuchte Episode umlegen und in die Liste in die richrige Reihenfolge darstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_val_printer(d):\n",
    "    for k, v in d.items():\n",
    "        print(v, k, sep=':')\n",
    "    #print()\n",
    "        \n",
    "def baum_printer(baeume: dict, baum: str):\n",
    "    key_val_printer (baeume[baum][0])\n",
    "    \n",
    "    \n",
    "    print(80*'-')\n",
    "    print(sum(baeume[baum][0].values()), baum)\n",
    "    print(80*'-')\n",
    "    key_val_printer(baeume[baum][1]) \n",
    "    \n",
    "            \n",
    "def wald_printer(wald: dict):\n",
    "    for baum in sorted(wald.keys()):\n",
    "        baum_printer (wald, baum)\n",
    "        print(80*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_val_printer(d):\n",
    "    for k, v in d.items():\n",
    "        print(v, k, sep=':')\n",
    "    #print()\n",
    "#def graph_printer(graph: dict):\n",
    "    #for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Die gesuchte Episode bestimmen !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = alle_aeste(\"'e300\", df)\n",
    "graph.update(alle_aeste(\"'e303\", df))\n",
    "wald_printer(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def cutoff_ast_data(wuerzel, graph, nachbar_liste, cutoff = 0):\n",
    "    \n",
    "    vor = graph[wuerzel][0]\n",
    "    zurueck = graph[wuerzel][1]\n",
    "    \n",
    "    nachbarn = vor.copy()\n",
    "    nachbarn.update(zurueck)\n",
    "     \n",
    "    for ast_name, ast_gewicht in sorted(nachbarn.items()):\n",
    "        if (ast_gewicht >= cutoff):\n",
    "             nachbar_liste.append([ep_name_format(wuerzel), ep_name_format(ast_name), ast_gewicht])\n",
    "            \n",
    "                \n",
    "def cutoff_ast_volldata(wuerzel, graph, nachbar_liste, cutoff = 0):\n",
    "    \n",
    "    vor = graph[wuerzel][0]\n",
    "    zurueck = graph[wuerzel][1]\n",
    "    \n",
    "    nachbarn = vor.copy()\n",
    "    nachbarn.update(zurueck)\n",
    "     \n",
    "    for ast_name, ast_gewicht in sorted(nachbarn.items()):\n",
    "        if (ast_gewicht >= cutoff):\n",
    "             nachbar_liste.append([ep_name_vollformat(wuerzel), ep_name_vollformat(ast_name), ast_gewicht])\n",
    "                \n",
    "                \n",
    "def cutoff_graph_data(graph, cutoff = 0):\n",
    "    nachbar_liste = []\n",
    "    for i, (k, v) in enumerate(sorted(graph.items())):\n",
    "        cutoff_ast_data(k, graph, nachbar_liste, cutoff)\n",
    "    neue_nachbar_liste = [[i, nachbar] for i, nachbar in enumerate(nachbar_liste)]\n",
    "    return neue_nachbar_liste\n",
    "\n",
    "def cutoff_graph_volldata(graph, cutoff = 0):\n",
    "    nachbar_liste = []\n",
    "    for i, (k, v) in enumerate(sorted(graph.items())):\n",
    "        cutoff_ast_volldata(k, graph, nachbar_liste, cutoff)\n",
    "    neue_nachbar_liste = [[i, nachbar] for i, nachbar in enumerate(nachbar_liste)]\n",
    "    return neue_nachbar_liste\n",
    "\n",
    "def interactive_graph_data(graph_data, loesch_index, gew_dict):\n",
    "    \n",
    "    for k, v in gew_dict.items():\n",
    "        graph_data[k][1][2] = v\n",
    "    neue_nachbar_liste = [[i, nachbar[1]] for i, nachbar in enumerate(graph_data) if i not in loesch_index]\n",
    "\n",
    "    return neue_nachbar_liste\n",
    "\n",
    "def graph_bauer(graph_data):\n",
    "    G = nx.Graph()\n",
    "    for el in graph_data:\n",
    "        n1 = el[1][0]\n",
    "        n2 = el[1][1]\n",
    "        w = el[1][2]\n",
    "        G.add_edge(n1, n2,weight=w)\n",
    "    return G\n",
    "\n",
    "def show_graph(G):\n",
    "    fh = open(\"edgelist.utf-8\", \"wb\")\n",
    "    nx.write_multiline_adjlist(G, fh, delimiter=\"\\t\", encoding=\"utf-8\")\n",
    "\n",
    "    # read and store in UTF-8\n",
    "\n",
    "    fh = open(\"edgelist.utf-8\", \"rb\")\n",
    "    H = nx.read_multiline_adjlist(fh, delimiter=\"\\t\", encoding=\"utf-8\")\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "    for p in pos:  # raise text positions\n",
    "        pos[p][0] += 0.07\n",
    "    nx.draw(G, pos, font_size=12, with_labels=True)\n",
    "\n",
    "    nx.draw_networkx_labels(G, pos, font_size=12, with_labels=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heufigkeit=4\n",
    "auto_graph=cutoff_graph_data(graph, heufigkeit)\n",
    "#auto_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interactive_data = interactive_graph_data(auto_graph,[],{})\n",
    "#print(*interactive_data, sep='\\n')\n",
    "G = graph_bauer(interactive_data)\n",
    "show_graph(G)\n",
    "\n",
    "mit_werte=cutoff_graph_volldata(graph, heufigkeit)\n",
    "mit_werte\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
